{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc478644-6c36-4cd2-baf1-26d71347a33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENBLAS_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"NUMEXPR_NUM_THREADS\"] = \"1\"\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "from sklearn.cluster import KMeans\n",
    "from joblib import Parallel, delayed, parallel_backend\n",
    "import multiprocessing\n",
    "import gc\n",
    "\n",
    "# Utility Functions\n",
    "def min_max_scale(data):\n",
    "    data = np.array(data, dtype=float) \n",
    "    data_min = np.min(data, axis=0)\n",
    "    data_max = np.max(data, axis=0)\n",
    "    return (data - data_min) / (data_max - data_min + 1e-9)\n",
    "\n",
    "def elbow_kmeans(data, max_k=5):\n",
    "    inertias = []\n",
    "    k_range = range(2, max_k + 1)\n",
    "    for k in k_range:\n",
    "        km = KMeans(n_clusters=k, n_init=2, random_state=42)\n",
    "        km.fit(data)\n",
    "        inertias.append(km.inertia_)\n",
    "    deltas = np.diff(inertias)\n",
    "    second_deriv = np.diff(deltas)\n",
    "    best_k = np.argmax(second_deriv) + 2\n",
    "    km_final = KMeans(n_clusters=best_k, n_init=3, random_state=42)\n",
    "    labels = km_final.fit_predict(data)\n",
    "    clusters = [[] for _ in range(best_k)]\n",
    "    for i, label in enumerate(labels):\n",
    "        clusters[label].append(i)\n",
    "    return clusters\n",
    "\n",
    "def Cluster_Retriever(original_matrix, group_a_idx, group_b_idx, max_k=10):\n",
    "    group_a_data = original_matrix[:, group_a_idx]\n",
    "    group_b_data = original_matrix[:, group_b_idx]\n",
    "    cluster_a = elbow_kmeans(group_a_data, max_k)\n",
    "    cluster_b = elbow_kmeans(group_b_data, max_k)\n",
    "    return cluster_a, cluster_b\n",
    "\n",
    "def MI_Calc(partition_P, partition_Q, n_samples):\n",
    "    H_P = 0.0\n",
    "    for cluster in partition_P:\n",
    "        p_i = len(cluster) / n_samples\n",
    "        if p_i > 0:\n",
    "            H_P += -p_i * np.log(p_i)\n",
    "    H_P_given_Q = 0.0\n",
    "    for q_cluster in partition_Q:\n",
    "        q_size = len(q_cluster)\n",
    "        p_q = q_size / n_samples\n",
    "        q_set = set(q_cluster)\n",
    "        for p_cluster in partition_P:\n",
    "            intersect_size = len(q_set.intersection(p_cluster))\n",
    "            if intersect_size > 0:\n",
    "                p_i_given_q = intersect_size / q_size\n",
    "                H_P_given_Q += -p_q * p_i_given_q * np.log(p_i_given_q)\n",
    "    return H_P - H_P_given_Q\n",
    "\n",
    "def Mutual_Information(Complete_Dataset, group_a_idx, group_b_idx, max_k_to_be_checked=10):\n",
    "    cluster_a, cluster_b = Cluster_Retriever(Complete_Dataset, group_a_idx, group_b_idx, max_k_to_be_checked)\n",
    "    score = MI_Calc(cluster_a, cluster_b, n_samples=Complete_Dataset.shape[0])\n",
    "    return score\n",
    "\n",
    "def compute_mi_joblib(partition):\n",
    "    group1 = [x - 1 for x in partition[1]]\n",
    "    group2 = [x - 1 for x in partition[2]]\n",
    "    cost = Mutual_Information(Sensor_Dataset, group1, group2, 4)\n",
    "    gc.collect()\n",
    "    return partition, cost\n",
    "\n",
    "# def create_ant_and_compute_mi(pheromone_matrix, index_matrix, sensor_data):\n",
    "#     print(\"New Ant\",'\\n')\n",
    "#     partition = {1: [], 2: []}\n",
    "#     while True:\n",
    "#         partitioner(pheromone_matrix, index_matrix)\n",
    "#         if len(partition[1]) + len(partition[2]) == Random_Matrix.shape[0]:\n",
    "#             break\n",
    "#     cost = Mutual_Information(sensor_data, [x - 1 for x in partition[1]], [x - 1 for x in partition[2]], 6)\n",
    "#     gc.collect()\n",
    "#     return partition, cost\n",
    "\n",
    "def partitioner(Matrix, Index_Matrix):\n",
    "    global partition\n",
    "    n = Matrix.shape[0]\n",
    "    idx = 1\n",
    "    index = []\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            index.append(idx)\n",
    "            idx += 1\n",
    "    probabilities = []\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            probabilities.append(Matrix[i][j] / Matrix.sum())\n",
    "            idx += 1\n",
    "    Index_Trail = np.random.choice(index, 1, p=probabilities)[0]\n",
    "    c = Index_Trail % n\n",
    "    if c == 0:\n",
    "        r, c = (Index_Trail // n, n)\n",
    "    else:\n",
    "        r, c = ((Index_Trail // n) + 1, Index_Trail % n)\n",
    "    sensor1, sensor2 = Nom_dict[Index_Matrix[r - 1][c - 1]]\n",
    "    if ((sensor1 not in partition[2]) and (sensor2 not in partition[1]) and\n",
    "        ((sensor1 not in partition[1]) or (sensor2 not in partition[2]))):\n",
    "        if sensor1 in partition[1]:\n",
    "            partition[2].append(sensor2)\n",
    "        elif sensor2 in partition[2]:\n",
    "            partition[1].append(sensor1)\n",
    "        else:\n",
    "            partition[1].append(sensor1)\n",
    "            partition[2].append(sensor2)\n",
    "\n",
    "def main():\n",
    "    global Pheromone_Matrix, ants, partition\n",
    "\n",
    "    # Load and prepare data\n",
    "    df1 = pd.read_csv(r\"C:\\Users\\Administrator\\Downloads\\2016WT6_ALL.csv\")\n",
    "    df2 = pd.read_csv(r\"C:\\Users\\Administrator\\Downloads\\2017WT6_ALL.csv\")\n",
    "\n",
    "    grd_power_cols = [col for col in df1.columns if col.startswith('Grd_') and 'Pwr' in col and col != 'Grd_Prod_Pwr_Avg']\n",
    "    min_max_std_cols = [col for col in df1.columns if any(stat in col for stat in ['_Min', '_Max', '_Std'])]\n",
    "    unwanted_wind_dir_cols = ['Amb_WindDir_Abs_Avg', 'Nac_Direction_Avg']\n",
    "    drop_cols = list(set(grd_power_cols + min_max_std_cols + unwanted_wind_dir_cols + ['Prod_LatestAvg_ActPwrGen2','Turbine_ID','Timestamp','Grd_Prod_PsbleInd_Avg', 'Grd_Prod_PsbleCap_Avg', 'Prod_LatestAvg_TotActPwr', 'Prod_LatestAvg_ReactPwrGen0', 'Prod_LatestAvg_ReactPwrGen1', 'Prod_LatestAvg_ReactPwrGen2', 'Prod_LatestAvg_TotReactPwr', 'Grd_Prod_Pwr_Avg', 'Grd_Prod_PsbleInd_Avg', 'Grd_Prod_PsbleCap_Avg', 'Amb_WindSpeed_Est_Avg', 'Prod_LatestAvg_TotActPwr']))\n",
    "\n",
    "    df1_reduced = df1.drop(columns=drop_cols)\n",
    "    df2_reduced = df2.drop(columns=drop_cols)\n",
    "    df2_reduced['Grd_Prod_CosPhi_Avg'] = df2_reduced['Grd_Prod_CosPhi_Avg'].fillna(df2_reduced['Grd_Prod_CosPhi_Avg'].mean())\n",
    "    df_reduced = pd.concat([df1_reduced, df2_reduced], axis=0)\n",
    "\n",
    "    global Sensor_Dataset\n",
    "    Sensor_Dataset = min_max_scale(df_reduced)\n",
    "\n",
    "    # Initialization\n",
    "    global Pheromone_Matrix, Random_Matrix, Index_Matrix, Nom_dict\n",
    "    Pheromone_Matrix = np.zeros((40, 40))\n",
    "    Random_Matrix = np.ones((40, 40))\n",
    "    np.fill_diagonal(Random_Matrix, 0)\n",
    "    Index_Matrix = np.zeros((40, 40))\n",
    "    Nom_dict = {}\n",
    "    idx = 1\n",
    "    for i in range(40):\n",
    "        for j in range(40):\n",
    "            Nom_dict[idx] = (i+1, j+1)\n",
    "            Index_Matrix[i][j] = idx\n",
    "            idx += 1\n",
    "\n",
    "    global ants\n",
    "    ants = []\n",
    "    global partition\n",
    "    partition = {1: [], 2: []}\n",
    "    n_ants = 200\n",
    "    n_jobs = min(multiprocessing.cpu_count(), 16)\n",
    "\n",
    "    with parallel_backend(\"threading\"):\n",
    "        while len(ants) < n_ants:\n",
    "            partition = {1: [], 2: []}\n",
    "            while len(partition[1]) + len(partition[2]) < Random_Matrix.shape[0]:\n",
    "                partitioner(Random_Matrix, Index_Matrix)\n",
    "            p1, p2 = sorted(partition[1]), sorted(partition[2])\n",
    "            if p1 > p2:\n",
    "                p1, p2 = p2, p1\n",
    "            norm = {1: p1, 2: p2}\n",
    "            if norm not in ants:\n",
    "                ants.append(copy.deepcopy(norm))\n",
    "        print(\"Initial Solution Guesses :\",'\\n')\n",
    "        for i in ants:\n",
    "            print(i,'\\n')\n",
    "\n",
    "        mi_results = Parallel(n_jobs=n_jobs,backend='loky')(\n",
    "            delayed(compute_mi_joblib)(ant) for ant in ants\n",
    "        )\n",
    "\n",
    "        for part, cost in mi_results:\n",
    "            for i in part[1]:\n",
    "                for j in part[2]:\n",
    "                    Pheromone_Matrix[i - 1][j - 1] += cost\n",
    "                    Pheromone_Matrix[j - 1][i - 1] += cost\n",
    "    print(\"Initial Pheromone Matrix :\",Pheromone_Matrix,'\\n')\n",
    "\n",
    "    # ACO Iterations\n",
    "    iterations = 500\n",
    "    best_cost = 0\n",
    "    best_partition = {}\n",
    "    max_costs = []\n",
    "    history = []\n",
    "    iterations_to_capture=[50,100,150,200,250,300,350,400,450,500]\n",
    "\n",
    "    for it in range(1, iterations + 1):\n",
    "        Pheromone_Matrix *= 0.65\n",
    "        ants=[]\n",
    "        costs=[]\n",
    "        with parallel_backend(\"threading\"):\n",
    "            while len(ants) < (n_ants-30):\n",
    "                partition = {1: [], 2: []}\n",
    "                while len(partition[1]) + len(partition[2]) < Random_Matrix.shape[0]:\n",
    "                    partitioner(Pheromone_Matrix, Index_Matrix)\n",
    "                p1, p2 = sorted(partition[1]), sorted(partition[2])\n",
    "                if p1 > p2:\n",
    "                    p1, p2 = p2, p1\n",
    "                norm = {1: p1, 2: p2}\n",
    "                if norm not in ants:\n",
    "                    ants.append(copy.deepcopy(norm))\n",
    "                    \n",
    "            while len(ants) < (n_ants):\n",
    "                partition = {1: [], 2: []}\n",
    "                while len(partition[1]) + len(partition[2]) < Random_Matrix.shape[0]:\n",
    "                    partitioner(Pheromone_Matrix, Index_Matrix)\n",
    "                p1, p2 = sorted(partition[1]), sorted(partition[2])\n",
    "                if p1 > p2:\n",
    "                    p1, p2 = p2, p1\n",
    "                norm = {1: p1, 2: p2}\n",
    "                if norm not in ants:\n",
    "                    ants.append(copy.deepcopy(norm))\n",
    "                    \n",
    "            mi_results = Parallel(n_jobs=n_jobs,backend='loky')(\n",
    "                delayed(compute_mi_joblib)(ant) for ant in ants\n",
    "                )\n",
    "            for part, cost in mi_results:\n",
    "                costs.append(cost)\n",
    "                for i in part[1]:\n",
    "                    for j in part[2]:\n",
    "                        Pheromone_Matrix[i - 1][j - 1] += cost\n",
    "                        Pheromone_Matrix[j - 1][i - 1] += cost\n",
    "                        \n",
    "        max_iter_cost = max(costs)\n",
    "        max_costs.append(max_iter_cost)\n",
    "        if max_iter_cost > best_cost:\n",
    "            best_cost = max_iter_cost\n",
    "            best_partition = ants[costs.index(max_iter_cost)]\n",
    "        history.append(best_cost)\n",
    "        print(f\"Iteration {it}: Max Cost = {max_iter_cost:.4f}, Best so far = {best_cost:.4f}\")\n",
    "        if it % 80 == 0:\n",
    "            noise_scale = 0.05 * np.mean(Pheromone_Matrix)\n",
    "            noise = np.random.normal(scale=noise_scale, size=Pheromone_Matrix.shape)\n",
    "            Pheromone_Matrix += noise\n",
    "         \n",
    "        if it in iterations_to_capture:\n",
    "            if it in iterations_to_capture:\n",
    "                plt.figure(figsize=(25, 25))\n",
    "                sns.heatmap(Pheromone_Matrix,\n",
    "                            annot=True,\n",
    "                            fmt=\".2f\",\n",
    "                            cmap='viridis',\n",
    "                            linewidths=0.5,\n",
    "                            linecolor='gray',\n",
    "                            cbar=True,\n",
    "                            square=True,\n",
    "                            annot_kws={\"size\": 10})\n",
    "                plt.xticks(fontsize=10, rotation=90)\n",
    "                plt.yticks(fontsize=10)\n",
    "                plt.tight_layout()\n",
    "                plt.savefig(f\"pheromone_matrix_iter_{it}.png\", dpi=300)\n",
    "                plt.close()\n",
    "\n",
    "        del ants\n",
    "        del costs\n",
    "        if it%5 ==0:\n",
    "            gc.collect()\n",
    "    print(\"Best Solution Found:\", best_partition)\n",
    "    print(\"Best Cost:\", best_cost)\n",
    "    plt.figure(figsize=(25,25))\n",
    "    plt.plot(history)\n",
    "    plt.xlabel(\"Iteration\")\n",
    "    plt.ylabel(\"Best Cost So Far\")\n",
    "    plt.title(\"Best Cost Function possible v/s iteration\")\n",
    "    plt.savefig(\"ACO_Convergence_1.png\")\n",
    "    plt.close()\n",
    "    plt.figure(figsize=(25,25))\n",
    "    plt.plot(max_costs)\n",
    "    plt.xlabel(\"Iteration\")\n",
    "    plt.ylabel(\"Best Cost So Far\")\n",
    "    plt.title(\"Best Solution at each iteration\")\n",
    "    plt.savefig(\"ACO_Convergence_2.png\")\n",
    "    plt.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
