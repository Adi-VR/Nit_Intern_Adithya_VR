{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "baed6fcd-0ea5-407f-bb2c-385ae75c654d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.11935023874038986\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "\n",
    "df=pd.read_csv(r\"C:\\Adithya\\IITK\\IITK EDU\\ML Intern\\2016WT6_ALL.csv\")\n",
    "\n",
    "keep_cols = [\n",
    "    'Grd_Prod_Pwr_Avg',\n",
    "    'Prod_LatestAvg_ActPwrGen0',\n",
    "    'Prod_LatestAvg_ActPwrGen1',\n",
    "    'Prod_LatestAvg_TotActPwr',\n",
    "    'Amb_WindDir_Relative_Avg'\n",
    "]\n",
    "\n",
    "# Keeping all columns that are not Grd based power readings except Avg total active power\n",
    "grd_power_cols = [col for col in df.columns \n",
    "                  if col.startswith('Grd_') and 'Pwr' in col and col != 'Grd_Prod_Pwr_Avg']\n",
    "\n",
    "# Dropping all columns with Min, Max, Std in name\n",
    "min_max_std_cols = [col for col in df.columns if any(stat in col for stat in ['_Min', '_Max', '_Std'])]\n",
    "\n",
    "# Dropping Nacelle and absolute wind direction columns\n",
    "unwanted_wind_dir_cols = ['Amb_WindDir_Abs_Avg', 'Nac_Direction_Avg']\n",
    "\n",
    "# Combining all columns to drop\n",
    "drop_cols = list(set(grd_power_cols + min_max_std_cols + unwanted_wind_dir_cols + ['Prod_LatestAvg_ActPwrGen2','Turbine_ID','Timestamp','Grd_Prod_PsbleInd_Avg', 'Grd_Prod_PsbleCap_Avg', 'Prod_LatestAvg_TotActPwr', 'Prod_LatestAvg_ReactPwrGen0', 'Prod_LatestAvg_ReactPwrGen1', 'Prod_LatestAvg_ReactPwrGen2', 'Prod_LatestAvg_TotReactPwr', 'Grd_Prod_Pwr_Avg', 'Grd_Prod_PsbleInd_Avg', 'Grd_Prod_PsbleCap_Avg', 'Amb_WindSpeed_Est_Avg', 'Prod_LatestAvg_TotActPwr']))\n",
    "\n",
    "# reduced DataFrame\n",
    "df_reduced = df.drop(columns=drop_cols)\n",
    "\n",
    "\n",
    "# df_reduced.info()\n",
    "cm=df_reduced.corr().abs()\n",
    "# plt.figure(figsize=(25,25))\n",
    "# sns.heatmap(cm, annot=True, cmap=\"coolwarm\", linewidths=.5)\n",
    "\n",
    "X=np.array(df_reduced)\n",
    "\n",
    "# Function to perform elbow-based KMeans clustering\n",
    "def elbow_kmeans(data, max_k=10):\n",
    "    inertias = []\n",
    "    for k in range(1, max_k + 1):\n",
    "        km = KMeans(n_clusters=k, n_init=10, random_state=42)\n",
    "        km.fit(data)\n",
    "        inertias.append(km.inertia_)\n",
    "\n",
    "    deltas = np.diff(inertias)\n",
    "    second_deriv = np.diff(deltas)\n",
    "    best_k = np.argmax(second_deriv) + 2  # +2 for second derivative offset\n",
    "\n",
    "    km_final = KMeans(n_clusters=best_k, n_init=10, random_state=42)\n",
    "    labels = km_final.fit_predict(data)\n",
    "\n",
    "    clusters = [[] for _ in range(best_k)]\n",
    "    for i, label in enumerate(labels):\n",
    "        clusters[label].append(i)\n",
    "\n",
    "    return clusters\n",
    "\n",
    "# Function 1: Partition columns, perform KMeans on both sides\n",
    "def function1(original_matrix, group_a_idx, group_b_idx, max_k=10):\n",
    "    group_a_data = original_matrix[:, group_a_idx]\n",
    "    group_b_data = original_matrix[:, group_b_idx]\n",
    "\n",
    "    cluster_a = elbow_kmeans(group_a_data, max_k)\n",
    "    cluster_b = elbow_kmeans(group_b_data, max_k)\n",
    "\n",
    "    return cluster_a, cluster_b\n",
    "\n",
    "# Function 2: Mutual information-like score H(P) - H(P|Q)\n",
    "def function2(partition_P, partition_Q, n_samples=None):\n",
    "\n",
    "    # Entropy H(P)\n",
    "    H_P = 0.0\n",
    "    for cluster in partition_P:\n",
    "        p_i = len(cluster) / n_samples\n",
    "        if p_i > 0:\n",
    "            H_P += -p_i * np.log(p_i)\n",
    "\n",
    "    # Conditional Entropy H(P|Q)\n",
    "    H_P_given_Q = 0.0\n",
    "    for q_cluster in partition_Q:\n",
    "        q_size = len(q_cluster)\n",
    "        p_q = q_size / n_samples\n",
    "        q_set = set(q_cluster)\n",
    "        for p_cluster in partition_P:\n",
    "            intersect_size = len(q_set.intersection(p_cluster))\n",
    "            if intersect_size > 0:\n",
    "                p_i_given_q = intersect_size / q_size\n",
    "                H_P_given_Q += -p_q * p_i_given_q * np.log(p_i_given_q)\n",
    "\n",
    "    return H_P - H_P_given_Q\n",
    "\n",
    "# Function 3: Complete pipeline\n",
    "def function3(Complete_Dataset, group_a_idx, group_b_idx, max_k_to_be_checked=10):\n",
    "    cluster_a, cluster_b = function1(Complete_Dataset, group_a_idx, group_b_idx, max_k_to_be_checked)\n",
    "    score = function2(cluster_a, cluster_b, n_samples=Complete_Dataset.shape[0])\n",
    "    return score\n",
    "\n",
    "partition={1:[1,3,5,7,9,10,11,12,16,18,20,29,30,32,34,37,39],2:[2,4,6,8,13,14,15,17,19,21,22,23,24,25,26,27,28,31,33,35,36,38,40]}\n",
    "MI=function3(X,[x - 1 for x in partition[1]],[x - 1 for x in partition[2]],50)\n",
    "print(MI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3393a1fe-644b-4577-bbf3-280a0212fe5f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
